{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_from_binary(year):\n",
    "    \"\"\"\n",
    "    Read in cleaned and trimmed data for SPY_{year} as a numpy array\n",
    "    Construct a pandas dataframe from the array\n",
    "    np.fromfile = 0.5 seconds\n",
    "    pd.read_csv = 30 seconds\n",
    "    \"\"\"\n",
    "    #read in the binary file \n",
    "    arr = np.fromfile('Polygon/Primed/SPY_{}.binary'.format(year), dtype=np.float64)\n",
    "    \n",
    "    #reshape the 1d array into 2d\n",
    "    arr = arr.reshape((int(len(arr)/3), 3))\n",
    "\n",
    "    #construct a Pandas dataframe\n",
    "    df  = pd.DataFrame(arr)\n",
    "    \n",
    "    #reassign the column names\n",
    "    df.columns = ['t', 'p', 's']\n",
    "    \n",
    "    #make the timestamp the index\n",
    "    df.set_index('t', inplace=True)\n",
    "   \n",
    "    #convert 64bit integer index to datetime (unit = nanoseconds)\n",
    "    df.index = df.index = pd.to_datetime(df.index, unit='ns')\n",
    "    \n",
    "    #convert index to US-Eastern timezone, automatically takes care of daylight savings\n",
    "    df.index = df.index.tz_localize('UTC').tz_convert('US/Eastern')\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def OHLCV(df, w):\n",
    "    \"\"\"\n",
    "    For the dataframe :df:, calculate the Open,High,Low,Close,Volume every period :w:\n",
    "    https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases\n",
    "    \"\"\"\n",
    "    #resample and calculate OHLCV -- range [w, w+1)\n",
    "    temp = df.resample(w, label='left',closed='left').agg({'p': 'ohlc', 's': 'sum'})\n",
    "    temp.columns = ['open','high','low','close','volume']\n",
    "    \n",
    "    #forward fill the close price to fill in any missing values\n",
    "    temp['close'].fillna(method='ffill', inplace=True)\n",
    "    temp['open'].fillna(temp['close'], inplace=True)\n",
    "    temp['high'].fillna(temp['close'], inplace=True)\n",
    "    temp['low'].fillna(temp['close'], inplace=True)\n",
    "    \n",
    "    return temp\n",
    "\n",
    "def bivariate(df, w):\n",
    "    \"\"\"\n",
    "    Calculate the last price and sum of shares traded in each period w\n",
    "    \"\"\"\n",
    "    #resample and calculate last price and sum of shares (volume) for each period :w:\n",
    "    #range [w, w+1)\n",
    "    temp = df.resample(w, label='left',closed='left').agg({'p':'last','s':'sum'})\n",
    "    \n",
    "    #forward fill the price for missing values\n",
    "    temp['p'].fillna(method='ffill', inplace=True)\n",
    "    \n",
    "    return temp\n",
    "\n",
    "def aggregate_trades(df, agg_type, w):\n",
    "\n",
    "    #get aggregated data for the eval_period\n",
    "    if agg_type == 'ohlcv':\n",
    "        agg_data = OHLCV(df, w)\n",
    "                \n",
    "    elif agg_type == 'bivariate':\n",
    "        agg_data = bivariate(df, w)\n",
    "    \n",
    "    #fill in zero shares with one share so that transform(df) doesn't divide by zero\n",
    "    agg_data.loc[agg_data['s'] < 1, 's'] = 1.0\n",
    "    \n",
    "    return agg_data\n",
    "    \n",
    "\n",
    "def transform(df):\n",
    "    \"\"\"\n",
    "    Convert prices and volume to percent different from previous\n",
    "    \"\"\"\n",
    "    #calculate percent difference from previous :w:\n",
    "    df['p'] = df['p'].pct_change()\n",
    "    df['s'] = df['s'].pct_change()\n",
    "    \n",
    "    #take natural log of returns to get normal distribution\n",
    "    df['s'] = np.log(1 + df['s'])\n",
    "    df['p'] = np.log(1 + df['p'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def strided_window(arr, window_size, step_size):\n",
    "    \"\"\"\n",
    "    Create expanded array of references that effectively create a \"memory\" for each row,\n",
    "    where columns are previous rows' data.\n",
    "    See generate_view() for information on what window_size and step_size are\n",
    "    https://stackoverflow.com/questions/40084931/taking-subarrays-from-numpy-array-with-given-stride-stepsize/40085052#40085052\n",
    "    \"\"\"\n",
    "    #needed to properly calculate stride sizes, can corrupt memory if incorrect\n",
    "    nrows = ((arr.size-window_size) // step_size) + 1\n",
    "    n     = arr.strides[0]\n",
    "    \n",
    "    #use striding tricks to create training data -- prevent writing to array to reduce chance of corruption\n",
    "    data_references = np.lib.stride_tricks.as_strided(arr, shape=(nrows,window_size), strides=(step_size*n,n), writeable=False)\n",
    "    \n",
    "    return data_references\n",
    "\n",
    "def generate_view(aggregated_data, lookback):\n",
    "    \"\"\"\n",
    "    Generates numpy view of ohlcv_data with shape (nrows-lookback, ncols*lookback)\n",
    "    :aggregated_data: a contiguous numpy array with shape (n,m)\n",
    "    :lookback: an integer specifying how many previous periods (w) to include in each row\n",
    "    \"\"\"\n",
    "    #increment lookback to produce expected behavior\n",
    "    lookback = lookback + 1\n",
    "    \n",
    "    ncols = aggregated_data.shape[1]\n",
    "    \n",
    "    #make the data a 1-dimensional array (unravel it)\n",
    "    data  = aggregated_data.ravel()\n",
    "    \n",
    "    #get views (references) of data (no copying, no extra memory)\n",
    "    data_strided = strided_window(data, window_size = lookback*ncols, step_size = ncols)\n",
    "    \n",
    "    return data_strided\n",
    "\n",
    "\n",
    "def parse_args(agg_type, w, lookback, eval_period):\n",
    "    \"\"\"\n",
    "    string args passed by tf.data.Dataset.from_generator are binary, must be decoded\n",
    "    also performs checks to make sure args are valid\n",
    "    \"\"\"\n",
    "    try:\n",
    "        agg_type    = agg_type.decode('utf-8').lower().strip()\n",
    "        w           = w.decode('utf-8')\n",
    "        eval_period = eval_period.decode('utf-8')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    assert agg_type in ['ohlcv', 'bivariate']\n",
    "    assert lookback >= 0\n",
    "\n",
    "    return agg_type, w, lookback, eval_period\n",
    "\n",
    "def data_generator(agg_type, w, lookback, eval_period, numpy_vals = True):\n",
    "    \"\"\"\n",
    "    Yield either OHLCV or bivariate data aggregated at w\n",
    "    Each yield is data for one eval_period\n",
    "    Reward (profit) should be assessed at end of each eval_period\n",
    "    microseconds(U) | milliseconds(L) | seconds(S) | minutes(T) | hour(H)\n",
    "    \"\"\"\n",
    "    agg_type, w, lookback, eval_period = parse_args(agg_type, w, lookback, eval_period)\n",
    "    \n",
    "    #go one year at a time to reduce memory usage -- reading in CSV's is a bottleneck\n",
    "    for year in range(2003,2005):\n",
    "        print(year)\n",
    "        \n",
    "        #read in the cleaned and trimmed trade data\n",
    "        spy = get_from_binary(year)\n",
    "        \n",
    "        #start iteration count\n",
    "        first_iteration = True\n",
    "        \n",
    "        #for each evaluation period (i.e. each game of pong) get aggregated data\n",
    "        for name, eval_period_data in spy.groupby(pd.Grouper(freq=eval_period)):\n",
    "\n",
    "            #if weekend or other time period with no trades\n",
    "            if eval_period_data.shape[0] <= lookback:\n",
    "                continue\n",
    "                \n",
    "            print('\\t{}'.format(name))                \n",
    "\n",
    "            ## get X ##\n",
    "            #get aggregated data\n",
    "            agg_data = aggregate_trades(eval_period_data, agg_type, w)\n",
    "\n",
    "            ## get Y ##\n",
    "            Prices = np.array(agg_data['p'].values[1:].astype(np.float32))\n",
    "\n",
    "            ## get Z ##\n",
    "            Mark = agg_data['p'].values[0]\n",
    "            \n",
    "            #transform the data (log returns)\n",
    "            agg_data = transform(agg_data)\n",
    "            agg_data = agg_data.iloc[1:,]\n",
    "\n",
    "            #yield Xtrain, Ytrain of previous window\n",
    "            if not first_iteration:\n",
    "                yield Xtrain, Ytrain, Mark\n",
    "            \n",
    "            #update to this window\n",
    "            Xtrain = generate_view(agg_data.values, lookback).astype(np.float32)\n",
    "            Ytrain = Prices.reshape((Prices.shape[0],1))\n",
    "            \n",
    "            first_iteration = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mygen1 = data_generator(agg_type    = 'bivariate', \n",
    "                        w           = 'S',\n",
    "                        lookback    = 0,\n",
    "                        eval_period = 'D',\n",
    "                        numpy_vals  = False)\n",
    "tmp = next(mygen1)[0]\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003\n",
      "\t2003-12-01 09:30:00-05:00\n",
      "\t2003-12-01 09:31:00-05:00\n",
      "[[ 0.0000000e+00 -4.4067192e+00]\n",
      " [ 7.4845011e-04  5.8774557e+00]\n",
      " [-2.8058531e-04 -1.9815620e+00]\n",
      " [ 0.0000000e+00 -1.3781972e+00]\n",
      " [ 2.8058531e-04  1.0376515e+00]]\n",
      "[[106.85]\n",
      " [106.93]\n",
      " [106.9 ]\n",
      " [106.9 ]\n",
      " [106.93]]\n",
      "106.93000030517578\n"
     ]
    }
   ],
   "source": [
    "mygen2 = data_generator(agg_type    = 'bivariate', \n",
    "                        w           = '10S',\n",
    "                        lookback    = 0,\n",
    "                        eval_period = 'min',\n",
    "                        numpy_vals  = True)\n",
    "Xt, Yt, Zt = next(mygen2)\n",
    "print(Xt)\n",
    "print(Yt)\n",
    "print(Zt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: how to get first price in next time period window? save previous data and execute with lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(tmp.p)\n",
    "fig.suptitle('Natural log of percent difference in prices over time', fontsize=16)\n",
    "plt.axis('off')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(tmp.s)\n",
    "fig.suptitle('Natural log of percent difference in volume over time', fontsize=16)\n",
    "plt.axis('off')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.hist(tmp.p, bins = 101)\n",
    "fig.suptitle('Histogram of natural log of percent difference in prices', fontsize=16)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('ln ( pct difference )', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.hist(tmp.s, bins = 109)\n",
    "fig.suptitle('Histogram of natural log of percent difference in volume', fontsize=16)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('ln ( pct difference )', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE      = 'bivariate'\n",
    "W         = 'T' #minute\n",
    "LOOKBACK  = 2\n",
    "EVAL      = 'D' #day\n",
    "\n",
    "\n",
    "bivariate_generator = tf.data.Dataset.from_generator(\n",
    "                    generator     = data_generator, args=[TYPE,W,LOOKBACK,EVAL], \n",
    "                    output_types  = (tf.float32, tf.float32),\n",
    "                    output_shapes =(tf.TensorShape([2*(LOOKBACK+1) if TYPE == 'bivariate' else 5*(LOOKBACK+1)]), tf.TensorShape([]))\n",
    "                    ).repeat().batch(8).prefetch(8)\n",
    "     \n",
    "price_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need an input specifying current position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try the Tensorflow custom training example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My trade data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test run output sentiments:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       "array([[0.9318945 ],\n",
       "       [0.63839996],\n",
       "       [0.6362751 ],\n",
       "       [0.48009208],\n",
       "       [0.1325867 ]], dtype=float32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(2,)),  # input shape required\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(1, activation=tf.nn.tanh)\n",
    "])\n",
    "print('Test run output sentiments:')\n",
    "model(Xt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Loss 1 (agnostic of current position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAvg Price: 106.89168548583984\n",
      "\tPosition: 2.8192481994628906\n",
      "\tProfit: 0.10801898688077927\n",
      "Loss: -0.10801898688077927\n"
     ]
    }
   ],
   "source": [
    "def loss(model, market, prices, mark, training, debug=True):\n",
    "    sentiments = model(market, training=training)\n",
    "    position   = tf.reduce_sum(sentiments)\n",
    "    avg_price  = tf.reduce_sum(tf.multiply(sentiments, prices)) / position\n",
    "    diff       = tf.subtract(mark, avg_price)\n",
    "    profit     = tf.multiply(diff, position)\n",
    "    loss       = -profit\n",
    "    if debug:\n",
    "        print('\\tAvg Price: {}'.format(avg_price))\n",
    "        print('\\tPosition: {}'.format(position))\n",
    "        print('\\tProfit: {}'.format(profit))\n",
    "    return loss\n",
    "print('Loss: {}'.format(loss(model, Xt, Yt, Zt, training=False, debug=True)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(model, market, prices, mark):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, market, prices, mark, training=True)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "\n",
    "def forward_pass_positions(model, inputs):\n",
    "    position   = 0\n",
    "    sentiments = np.zeros((inputs.shape[0]))\n",
    "    \n",
    "    for i in range(0,inputs.shape[0]-1):\n",
    "        #get sentiment for single observation (position = 0)\n",
    "        s         = model(inputs[i-1])[0][0] \n",
    "        position += s\n",
    "        \n",
    "        #fill in position for next observation\n",
    "        inputs[i+1][0][2] = position\n",
    "        sentiments[i]      = s\n",
    "        \n",
    "    #get sentiment for last observation\n",
    "    sentiments[-1] = model(inputs[-1])[0][0] \n",
    "    return sentiments\n",
    "\n",
    "def custom_grad(model, inputs, prices):\n",
    "    #do non-differentiable work here:\n",
    "    sentiments = forward_pass_positions(model, inputs)\n",
    "    \n",
    "    #differentiable work (calculate gradients)\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = custom_loss_2(model, sentiments, prices, training=True)\n",
    "        \n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAvg Price: 106.89168548583984\n",
      "\tPosition: 2.8192481994628906\n",
      "\tProfit: 0.10801898688077927\n",
      "Step: 0, Initial Loss: -0.10801898688077927\n",
      "\tAvg Price: 106.89189147949219\n",
      "\tPosition: 2.920367479324341\n",
      "\tProfit: 0.11129177361726761\n",
      "Step: 1,         Loss: -0.11129177361726761\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "#Example of training step\n",
    "loss_value, grads = grad(model, Xt, Yt, Zt) #last arg is labels\n",
    "\n",
    "print(\"Step: {}, Initial Loss: {}\".format(optimizer.iterations.numpy(),\n",
    "                                          loss_value.numpy()))\n",
    "\n",
    "optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "\n",
    "print(\"Step: {},         Loss: {}\".format(optimizer.iterations.numpy(),loss(model, Xt, Yt, Zt, training=True).numpy()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAvg Price: 106.89189147949219\n",
      "\tPosition: 2.920367479324341\n",
      "\tProfit: 0.11129177361726761\n",
      "Epoch 001: Loss: -0.111\n",
      "\tAvg Price: 106.8921127319336\n",
      "\tPosition: 3.0059540271759033\n",
      "\tProfit: 0.11388830095529556\n",
      "Epoch 002: Loss: -0.114\n",
      "\tAvg Price: 106.89228820800781\n",
      "\tPosition: 3.0796096324920654\n",
      "\tProfit: 0.11613854020833969\n",
      "Epoch 003: Loss: -0.116\n",
      "\tAvg Price: 106.8924789428711\n",
      "\tPosition: 3.143947124481201\n",
      "\tProfit: 0.11796517670154572\n",
      "Epoch 004: Loss: -0.118\n",
      "\tAvg Price: 106.89262390136719\n",
      "\tPosition: 3.200815439224243\n",
      "\tProfit: 0.1196349710226059\n",
      "Epoch 005: Loss: -0.120\n",
      "\tAvg Price: 106.89279174804688\n",
      "\tPosition: 3.251528263092041\n",
      "\tProfit: 0.12098467350006104\n",
      "Epoch 006: Loss: -0.121\n",
      "\tAvg Price: 106.8929443359375\n",
      "\tPosition: 3.2971112728118896\n",
      "\tProfit: 0.12217765301465988\n",
      "Epoch 007: Loss: -0.122\n",
      "\tAvg Price: 106.89308166503906\n",
      "\tPosition: 3.3383612632751465\n",
      "\tProfit: 0.12324775755405426\n",
      "Epoch 008: Loss: -0.123\n",
      "\tAvg Price: 106.8931884765625\n",
      "\tPosition: 3.3759098052978516\n",
      "\tProfit: 0.12427341192960739\n",
      "Epoch 009: Loss: -0.124\n",
      "\tAvg Price: 106.89331817626953\n",
      "\tPosition: 3.410264253616333\n",
      "\tProfit: 0.12509575486183167\n",
      "Epoch 010: Loss: -0.125\n",
      "\tAvg Price: 106.89341735839844\n",
      "\tPosition: 3.4418396949768066\n",
      "\tProfit: 0.1259126365184784\n",
      "Epoch 011: Loss: -0.126\n",
      "\tAvg Price: 106.89352416992188\n",
      "\tPosition: 3.4709789752960205\n",
      "\tProfit: 0.12660789489746094\n",
      "Epoch 012: Loss: -0.127\n",
      "\tAvg Price: 106.89361572265625\n",
      "\tPosition: 3.4979686737060547\n",
      "\tProfit: 0.1272721290588379\n",
      "Epoch 013: Loss: -0.127\n",
      "\tAvg Price: 106.89370727539062\n",
      "\tPosition: 3.5230493545532227\n",
      "\tProfit: 0.1278621405363083\n",
      "Epoch 014: Loss: -0.128\n",
      "\tAvg Price: 106.893798828125\n",
      "\tPosition: 3.546426773071289\n",
      "\tProfit: 0.12838588654994965\n",
      "Epoch 015: Loss: -0.128\n",
      "\tAvg Price: 106.89387512207031\n",
      "\tPosition: 3.5682764053344727\n",
      "\tProfit: 0.12890464067459106\n",
      "Epoch 016: Loss: -0.129\n",
      "\tAvg Price: 106.89395141601562\n",
      "\tPosition: 3.588749885559082\n",
      "\tProfit: 0.12937045097351074\n",
      "Epoch 017: Loss: -0.129\n",
      "\tAvg Price: 106.89402770996094\n",
      "\tPosition: 3.6079797744750977\n",
      "\tProfit: 0.12978839874267578\n",
      "Epoch 018: Loss: -0.130\n",
      "\tAvg Price: 106.89409637451172\n",
      "\tPosition: 3.62608003616333\n",
      "\tProfit: 0.13019052147865295\n",
      "Epoch 019: Loss: -0.130\n",
      "\tAvg Price: 106.89417266845703\n",
      "\tPosition: 3.64315128326416\n",
      "\tProfit: 0.13052549958229065\n",
      "Epoch 020: Loss: -0.131\n",
      "\tAvg Price: 106.89421844482422\n",
      "\tPosition: 3.6592841148376465\n",
      "\tProfit: 0.13093599677085876\n",
      "Epoch 021: Loss: -0.131\n",
      "\tAvg Price: 106.894287109375\n",
      "\tPosition: 3.6745548248291016\n",
      "\tProfit: 0.13123010098934174\n",
      "Epoch 022: Loss: -0.131\n",
      "\tAvg Price: 106.89433288574219\n",
      "\tPosition: 3.688494920730591\n",
      "\tProfit: 0.1315590888261795\n",
      "Epoch 023: Loss: -0.132\n",
      "\tAvg Price: 106.8943862915039\n",
      "\tPosition: 3.700796604156494\n",
      "\tProfit: 0.13180021941661835\n",
      "Epoch 024: Loss: -0.132\n",
      "\tAvg Price: 106.89444732666016\n",
      "\tPosition: 3.7126007080078125\n",
      "\tProfit: 0.13199400901794434\n",
      "Epoch 025: Loss: -0.132\n",
      "\tAvg Price: 106.89449310302734\n",
      "\tPosition: 3.723912477493286\n",
      "\tProfit: 0.13222570717334747\n",
      "Epoch 026: Loss: -0.132\n",
      "\tAvg Price: 106.89454650878906\n",
      "\tPosition: 3.734750509262085\n",
      "\tProfit: 0.13241107761859894\n",
      "Epoch 027: Loss: -0.132\n",
      "\tAvg Price: 106.89459228515625\n",
      "\tPosition: 3.7451837062835693\n",
      "\tProfit: 0.1326095461845398\n",
      "Epoch 028: Loss: -0.133\n",
      "\tAvg Price: 106.8946304321289\n",
      "\tPosition: 3.7552356719970703\n",
      "\tProfit: 0.1328222155570984\n",
      "Epoch 029: Loss: -0.133\n",
      "\tAvg Price: 106.8946762084961\n",
      "\tPosition: 3.764927864074707\n",
      "\tProfit: 0.1329926699399948\n",
      "Epoch 030: Loss: -0.133\n",
      "\tAvg Price: 106.89472961425781\n",
      "\tPosition: 3.774190664291382\n",
      "\tProfit: 0.13311831653118134\n",
      "Epoch 031: Loss: -0.133\n",
      "\tAvg Price: 106.89476776123047\n",
      "\tPosition: 3.7829549312591553\n",
      "\tProfit: 0.1332831233739853\n",
      "Epoch 032: Loss: -0.133\n",
      "\tAvg Price: 106.89479064941406\n",
      "\tPosition: 3.7914488315582275\n",
      "\tProfit: 0.13349561393260956\n",
      "Epoch 033: Loss: -0.133\n",
      "\tAvg Price: 106.89483642578125\n",
      "\tPosition: 3.799684762954712\n",
      "\tProfit: 0.13361166417598724\n",
      "Epoch 034: Loss: -0.134\n",
      "\tAvg Price: 106.89488220214844\n",
      "\tPosition: 3.8076744079589844\n",
      "\tProfit: 0.13371829688549042\n",
      "Epoch 035: Loss: -0.134\n",
      "\tAvg Price: 106.89491271972656\n",
      "\tPosition: 3.815429925918579\n",
      "\tProfit: 0.13387422263622284\n",
      "Epoch 036: Loss: -0.134\n",
      "\tAvg Price: 106.89494323730469\n",
      "\tPosition: 3.82295560836792\n",
      "\tProfit: 0.1340216100215912\n",
      "Epoch 037: Loss: -0.134\n",
      "\tAvg Price: 106.89498138427734\n",
      "\tPosition: 3.8302626609802246\n",
      "\tProfit: 0.13413166999816895\n",
      "Epoch 038: Loss: -0.134\n",
      "\tAvg Price: 106.89501190185547\n",
      "\tPosition: 3.8373661041259766\n",
      "\tProfit: 0.1342633068561554\n",
      "Epoch 039: Loss: -0.134\n",
      "\tAvg Price: 106.89505767822266\n",
      "\tPosition: 3.8442723751068115\n",
      "\tProfit: 0.13432897627353668\n",
      "Epoch 040: Loss: -0.134\n",
      "\tAvg Price: 106.89507293701172\n",
      "\tPosition: 3.850992202758789\n",
      "\tProfit: 0.1345050185918808\n",
      "Epoch 041: Loss: -0.135\n",
      "\tAvg Price: 106.8951187133789\n",
      "\tPosition: 3.8575313091278076\n",
      "\tProfit: 0.1345568299293518\n",
      "Epoch 042: Loss: -0.135\n",
      "\tAvg Price: 106.8951416015625\n",
      "\tPosition: 3.863898277282715\n",
      "\tProfit: 0.13469047844409943\n",
      "Epoch 043: Loss: -0.135\n",
      "\tAvg Price: 106.89517974853516\n",
      "\tPosition: 3.8700995445251465\n",
      "\tProfit: 0.13475902378559113\n",
      "Epoch 044: Loss: -0.135\n",
      "\tAvg Price: 106.89519500732422\n",
      "\tPosition: 3.8761415481567383\n",
      "\tProfit: 0.13491025567054749\n",
      "Epoch 045: Loss: -0.135\n",
      "\tAvg Price: 106.89523315429688\n",
      "\tPosition: 3.8820302486419678\n",
      "\tProfit: 0.1349671334028244\n",
      "Epoch 046: Loss: -0.135\n",
      "\tAvg Price: 106.89527130126953\n",
      "\tPosition: 3.8877718448638916\n",
      "\tProfit: 0.13501843810081482\n",
      "Epoch 047: Loss: -0.135\n",
      "\tAvg Price: 106.89527893066406\n",
      "\tPosition: 3.8933727741241455\n",
      "\tProfit: 0.13518325984477997\n",
      "Epoch 048: Loss: -0.135\n",
      "\tAvg Price: 106.89532470703125\n",
      "\tPosition: 3.898837089538574\n",
      "\tProfit: 0.13519451022148132\n",
      "Epoch 049: Loss: -0.135\n",
      "\tAvg Price: 106.89533233642578\n",
      "\tPosition: 3.904170274734497\n",
      "\tProfit: 0.13534964621067047\n",
      "Epoch 050: Loss: -0.135\n"
     ]
    }
   ],
   "source": [
    "## Note: Rerunning this cell uses the same model variables\n",
    "\n",
    "# Keep results for plotting\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "\n",
    "    # Training loop - using batches of batch_size\n",
    "    #for x, y in train_dataset:\n",
    "        \n",
    "        #print('x: {}'.format(x))\n",
    "        #print('y: {}'.format(y))\n",
    "        \n",
    "    # Optimize the model\n",
    "    loss_value, grads = grad(model, Xt, Yt, Zt)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Track progress\n",
    "    epoch_loss_avg(loss_value)  # Add current batch loss\n",
    "\n",
    "    # End epoch\n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "\n",
    "    #if epoch % 50 == 0:\n",
    "    print('Epoch {:03d}: Loss: {:.3f}'.format(epoch+1, epoch_loss_avg.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
